# -*- coding: utf-8 -*-
"""MAIR Group 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GU5ZxJqwHSkRM6dDSaTw-rZN85aKHSK7

# Baseline systems
"""

# import library's 
import pandas as pd
import numpy as np
import sklearn.metrics
from tensorflow import keras
from sklearn.metrics import accuracy_score
from matplotlib import pyplot as plt
from mlxtend.evaluate import confusion_matrix
from mlxtend.plotting import plot_confusion_matrix
import random
import seaborn as sn
from io import StringIO
from collections import Counter

#Import data file
df = pd.read_csv(('dialog_acts.dat'),names=['label'])

# Transform to lowercase
df['label'] = df['label'].str.lower()
df[['label','utterance']] = df['label'].str.split(" ",1,expand=True)

# split into 85% train and 15% test data
mask = np.random.rand(len(df)) <= 0.85
training_data = df[mask]
testing_data = df[~mask]

"""**Majority class baseline system**"""

# Function that uses testing data as input and classifies as most_common label
def majority_baseline(testing_data):
  # Copy testing_data dataframe
  df_labels = pd.DataFrame.copy(testing_data)
  # Find most common label
  most_common = testing_data['label'].value_counts().idxmax()

  # Fill all rows of df dataframe with most common label
  df_labels[:]['label'] = most_common
  print(testing_data)
  return df_labels['label']

# compare predicted labels with correct labels in testing_data
accuracy_score(testing_data['label'],majority_baseline(testing_data))

"""**Rule based baseline system**"""

# Function that uses testing_data as input and classifies using keyword matching
def keyword_matching(testing_data):
  # Copy testing_data dataframe
  df_labels = pd.DataFrame.copy(testing_data)
  # Create empty array for storing predicted labels
  keyword_label = []

  # Iterate through testing_data dataframe and check every row's keyword
  for row in range(len(df_labels)):
    word = str(df_labels['utterance'].iloc[[row]])
    keyword_label.append(dialog_type(word))

  # Fill df_labels dataframe with predicted labels
  df_labels[:]['label'] = keyword_label

  return df_labels['label']

# Function to find type of dialog in a string
def dialog_type(utterance):
  if utterance.find("okay") > 0 or utterance.find("oke") > 0:
    keyword = "ack"
  elif utterance.find("yes") > 0 or utterance.find("right") > 0:
    keyword = "affirm"
  elif utterance == "no":
    keyword = "negate"
  elif utterance.find("how about") > 0 or utterance.find("what about") > 0 or utterance.find("else") > 0:
    keyword = "reqalts"
  elif utterance.find("thank you") > 0 or utterance.find("thankyou") > 0 or utterance.find("thanks") > 0:
    keyword = "thankyou"
  elif utterance.find("bye") > 0:
    keyword = "bye"
  elif utterance.find("start") > 0:
    keyword = "restart"
  elif utterance.find("what") > 0 or utterance.find("how") > 0 or utterance.find("address") > 0 or utterance.find("number") > 0 or utterance.find("postcode") > 0:
    keyword = "request"
  elif utterance.find("more") > 0:
    keyword = "reqmore"
  elif utterance.find("repeat") > 0:
    keyword = "repeat"
  elif utterance.find("hi") > 0 or utterance.find("hello") > 0:
    keyword = "hello" 
  elif utterance.find("don't") > 0 or utterance.find("dont") > 0:
    keyword = "deny"
  elif utterance.find("is it") > 0 or utterance == "is":
    keyword = "confirm"
  elif utterance.find("looking") > 0 or utterance.find("want") > 0 or utterance.find("would") > 0 or utterance.find("any") > 0 or utterance.find("restaurant") > 0 or utterance.find("food") > 0:
    keyword = "inform"
  elif len(utterance) < 7 or utterance.find("noise") > 0 or utterance.find('sil') > 0:
    keyword = "null"
  else:
    keyword = "inform"

  return keyword

# compare predicted labels with correct labels in testing_data
print(accuracy_score(testing_data['label'],keyword_matching(testing_data)))

# All possible response types (15 types)
list_labels = ["ack","affirm","negate","inform","thankyou","bye", "restart","request","reqmore",  "reqalts", "repeat", "hello" ,"deny","confirm", "null"]

# Compare the prediction of keyword matching model with actual labels

# Get keyword model's prediction
pred_key = keyword_matching(testing_data)

# Actual labels
test_key = testing_data['label']

# Sentences of testing_data
inpt = testing_data['utterance']

# Compare sentence by sentence if a prediction was correct
for row_index, (inpt, prediction, label) in enumerate( zip (inpt, pred_key, test_key)):
  # If prediction incorrect, print the sentence and misclassification type
  if prediction != label:
    print('Row', row_index, 'has been classified as ', prediction, 'and should be ', label, '\t', '\t', inpt)

"""#Machine learning

## Model 1: Neural Network
"""

# Create neural network model
# Import library's
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder

# Copy train and test data for this model
m1_sentences_train = training_data['utterance']
m1_sentences_test = testing_data['utterance']
m1_y_train = training_data['label']
m1_y_test = testing_data['label']

# Vectorize the training and testing x data
m1_vectorizer = CountVectorizer()
m1_vectorizer.fit(m1_sentences_train)

m1_x_train = m1_vectorizer.transform(m1_sentences_train)
m1_x_test = m1_vectorizer.transform(m1_sentences_test)

# Categorize the labels (y data)
label_encoder = LabelEncoder()
m1_y_train = label_encoder.fit_transform(m1_y_train)
m1_y_test = label_encoder.fit_transform(m1_y_test)

m1_y_train = keras.utils.to_categorical(m1_y_train, 18)
m1_y_test = keras.utils.to_categorical(m1_y_test, 18)

# Test neural network
from keras.models import Sequential 
from keras import layers
from keras.backend import clear_session

m1_input_dim = m1_x_train.shape[1]

# Architecture - layers
model1 = Sequential()
model1.add(layers.Dense(28, input_dim=m1_input_dim, activation = 'relu'))
model1.add(layers.Dense(18, activation='sigmoid'))
model1.compile(loss='binary_crossentropy', optimizer='adam', metrics='accuracy')

# Model summary
model1.summary()

# Train model and print training epochs
history = model1.fit(m1_x_train, m1_y_train, epochs=15, verbose=1 , validation_data=(m1_x_test, m1_y_test), batch_size=100)

#clear_session()

# Get training and validation accuracy for neural network
model1_training_loss, model1_training_accuracy = model1.evaluate(m1_x_train, m1_y_train, verbose=False)
print("Training Accuracy: ",  "{:.0%}".format(model1_training_accuracy), " Training loss: ", "{:.0%}". format(model1_training_loss))

model1_validation_loss, model1_validation_accuracy = model1.evaluate(m1_x_test, m1_y_test, verbose=False)
print("Validation Accuracy: ", "{:.0%}".format(model1_validation_accuracy), " Validation loss: ", "{:.0%}". format(model1_validation_loss))

m1_y_pred=model1.predict(m1_x_test) 
m1_y_pred=np.argmax(m1_y_pred, axis=1)
y_test_eva =np.argmax(m1_y_test, axis=1)

# Plot the training (validation) accuracy and history
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Acuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='right')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='right')
plt.show()

# Calculate loss and accuracy
loss, accuracy = model1.evaluate(m1_x_test, m1_y_test, verbose=0)
print(loss)
print(accuracy)

#input prompt for the first model
'''
while(True):
  pred_sentence = str(input("Enter new sentence : "))
  pred_sentence = m1_vectorizer.transform([pred_sentence])
  y_new = model1.predict([pred_sentence])
  y_class = y_new.argmax(axis=-1)
  print(sorted(list_labels)[y_class[0]])
'''

"""## Model 2: Logistic Regression"""

# Model 2: Logistic regression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression

# Prepare data - vectorize
m2_sentences = df['utterance'].values
m2_y = df['label'].values
m2_sentences_train, m2_sentences_test, m2_y_train, m2_y_test = train_test_split(m2_sentences, m2_y, test_size=0.15, random_state=1000)
m2_vectorizer = CountVectorizer()
m2_vectorizer.fit(m2_sentences_train)
m2_x_train = m2_vectorizer.transform(m2_sentences_train)
m2_x_test  = m2_vectorizer.transform(m2_sentences_test)

# Fit regression model
logisticRegr = LogisticRegression(max_iter = 250)
logisticRegr.fit(m2_x_train, m2_y_train)

# Predict one input point
logisticRegr.predict(m2_x_test[0].reshape(1,-1))
logisticRegr.predict(m2_x_test[0:10])
predictions = logisticRegr.predict(m2_x_test)

# Get accuracy
score = logisticRegr.score(m2_x_test, m2_y_test)
print(score)

#input prompt for the second model
'''
while(True):
  pred_sentence = input("Enter new sentence : ")
  pred_sentence = m2_vectorizer.transform([pred_sentence])
  y_class = logisticRegr.predict(pred_sentence)
  print(y_class)
'''

"""# Evaluation

Quantitative evaluation:

Accuracy, Precision, and Recall: 
F1 Score: 

Error analysis:
- Maybe Confusion metrics? List of errors per sentences

"""



"""## Model 1: Neural network

### Quantitative evaluation

We chose the accuracy as a general overview for quantitive evaluation because it gives a good estimate of how well the model performs. Also the F1 score because if the data is heavily skewed, then the F1 gives a good estimate.
"""

# Neural network quantative evaluation
# plot training history
plt.figure(figsize=(9,6))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy Vs Epochs')
plt.ylabel('Accuracy')
plt.xlabel('Number of Epochs')
plt.legend(['Training Accuracy', 'Validation Accuracy'])
plt.show()

# Calculate training and validation accuracy
model1_training_loss, model1_training_accuracy = model1.evaluate(m1_x_train, m1_y_train, verbose=False)
print("Training Accuracy: ",  "{:.0%}".format(model1_training_accuracy), " Training loss: ", "{:.0%}". format(model1_training_loss))

model1_validation_loss, model1_validation_accuracy = model1.evaluate(m1_x_test, m1_y_test, verbose=False)
print("Validation Accuracy: ", "{:.0%}".format(model1_validation_accuracy), " Validation loss: ", "{:.0%}". format(model1_validation_loss))

# Confusion Matrix to see where things go wrong
print("Confusion Matrix of Model 1")
cm = confusion_matrix(y_test_eva, m1_y_pred)
plt.figure(figsize = (15,12))
sn.heatmap(cm, annot=True,cmap="OrRd",fmt='g')
# fig, ax = plot_confusion_matrix(cm, figsize=(8,8),)
# plt.title('Confusion Matrix')
# plt.show()

# Calculate F-score - precision and recall
from sklearn.metrics import precision_recall_fscore_support
precision_recall_fscore_support(y_test_eva, m1_y_pred, average='macro')

"""So precision is 0.63 and recall is 0.60 percent. This is not very high as we will see later on in comparison with the logistic regression.

### Error analysis
"""

# Finding items that are most often classified wrong
inpt = testing_data['utterance']
wrong_utterance = [] 
for row_index, (inpt, prediction, label) in enumerate( zip (inpt, m1_y_pred, y_test_eva)):
  if prediction != label:
    wrong_utterance.append(inpt)

Counter(wrong_utterance).most_common()[0:15]

"""The Model 1 Neural network classifier finds it hard to classify 'okay thank you and good bye' which is understandable since it contains 3 different commands: a confirmation, thankyou and goodbye. Especially commands containing a form of confirmation 'okay' without being a confirmation are hard.
It also classifies a couple of other cases wrong, but since the accuracy is really high, these are just single instances.

### Difficult cases

***TODO: for example utterances that are not fluent (e.g. due to speech recognition issues) or the presence of negation (I donâ€™t want an expensive restaurant). For each case, create test instances and evaluate how your systems perform on these cases. Such as:***

### System comparison

The neural network model compares well against the baseline. The validation accuracy is 98 percent compared to 39 percent majority and 84 percent keyword baseline systems. Therefore, the neural network is much more accurate than the baseline systems with an improvement of at least 14 percent accuracy. For the comparison of a neural network model 1 and model 2 logistic regression, see the model 2 section on system comparison.

## Model 2: Logistic Regression

### Quantative evaluation
"""

# Import libraries
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics

# Create confusion matrix
cm = metrics.confusion_matrix(m2_y_test, predictions)
plt.figure(figsize = (15,12))
sn.heatmap(cm, annot=True,cmap="OrRd",fmt='g')

# Calculate F1 score - precision and recall
precision_recall_fscore_support(m2_y_test, predictions, average='macro')

"""So precision is 0.91 and recall is 0.81 percent.

### Error analysis
"""

#Finding items that are most often classified wrong
inpt = testing_data['utterance']
wrong_utterance = [] 
for row_index, (inpt, prediction, label) in enumerate( zip (inpt, predictions, m2_y_test)):
  if prediction != label:
    wrong_utterance.append(inpt)

Counter(wrong_utterance).most_common()[0:15]

"""The model 2 logistic regression classifies the 'i don't care' and the short answers containing one word often wrong. It also finds it hard to classify the sentence containing both a thankyou and goodbye, just like model 1. It can be imagined that this is indeed difficult for a model because there are two commands and thus the sentence classifies equally well as both categories.

### Difficult cases

### System comparison

The logistic regression performs well compared to the baseline models. Both baseline models have lower accuracy and are therefore not great classifiers. The logistic regression reaches an accuracy of 98 percent which is a good performance. This model would be better suited to classify dialog than the baseline models.

The neural network has the same accuracy as the logistic regression model, with both having a 98 percent prediction accuracy in classifying dialog. Interestingly, both models also both have difficulty classifying sentences containing two commands. However, when we look at the precision and recall score resulting from the F1 analysis, it turns out that the logistic regression has a much higher percentage in both precision and recall than the neural network. Therefore, we conclude that the logistic regression would be better suited to classify dialog than the neural network.  TODO: add difficult cases evaluation
"""

# Difficult cases test
import pandas as pd
from sklearn import model_selection
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import log_loss
from sklearn.metrics import precision_recall_fscore_support as score, precision_score, recall_score, f1_score
from sklearn import preprocessing
names = ['ack']
dataframe = pd.read_csv('dialog_acts.dat', names=names)
array = dataframe.values
test_size = 0.33
X_train, X_test, Y_train, Y_test = train_test_split(dataframe,array, train_size = 0.85)
Y_test= Y_test['ack']
print(Y_test.head())
# model = LogisticRegression(solver='liblinear')
# model= preprocessing.LabelEncoder()
# model.fit_transform(X_train, Y_train)
# predicted = model.predict(X_test)
# report = classification_report(Y_test, predicted)
# print(report)

"""# Part 1B

## Keyword matching
Receives user input string and finds keywords
"""

# Determines levenshteinDistance between two elements
import numpy as np

# Calculate likeness of two words with levenshtein distance
def levenshteinDistanceMatrix(token1, token2):
    distances = np.zeros((len(token1) + 1, len(token2) + 1))
    for t1 in range(len(token1) + 1):
        distances[t1][0] = t1
    for t2 in range(len(token2) + 1):
        distances[0][t2] = t2
        
    a = 0
    b = 0
    c = 0
    
    # Calculate distance between token 1 and 2
    for t1 in range(1, len(token1) + 1):
        for t2 in range(1, len(token2) + 1):
            if (token1[t1-1] == token2[t2-1]):
                distances[t1][t2] = distances[t1 - 1][t2 - 1]
            else:
                a = distances[t1][t2 - 1]
                b = distances[t1 - 1][t2]
                c = distances[t1 - 1][t2 - 1]
                
                if (a <= b and a <= c):
                    distances[t1][t2] = a + 1
                elif (b <= a and b <= c):
                    distances[t1][t2] = b + 1
                else:
                    distances[t1][t2] = c + 1

    return distances[len(token1)][len(token2)]

# Find the words that are closest to the input word
def closest_area(sentence,dictionary):

  # Will store all words and distances if < 3
  dictWordDist = []

  # Go through every word in sentence
  for word in sentence:
    if len(word) > 3:
      wordIdx = 0
        
      # Check the word against every dictionary element
      for line in dictionary: 
          wordDistance = levenshteinDistanceMatrix(word, line.strip())
          if wordDistance >= 10:
              wordDistance = 9
          # Only include words with distance less then three
          if wordDistance < 3:
            dictWordDist.append(str(int(wordDistance)) + "-" + line.strip())
            wordIdx = wordIdx + 1

  # Go through all distances found for every word in sentence
  closestWords = []

  # If there are suitable words found, sort and find the closest word
  if len(dictWordDist) > 0:
    wordDetails = []
    currWordDist = 0

    # Sort list
    dictWordDist.sort()

    # Select shortest distance match
    currWordDist = dictWordDist[0]
    wordDetails = currWordDist.split("-")
    closestWords.append(wordDetails[1])
    
    # For converting list element to string, make empty string and join element in
    return_str = ""
    return return_str.join(closestWords)
  else:
    return None

# Function to find keyword user preferences for area, food and price in string
# Utterance has to be a list of string words
def keyword(utterance):

  # Find area
  area = []

  # Possible areas
  dictionary_area = ["north","south","east","west","centre"]

  for word in utterance:
    # If input word matches one in dictionary, then succesful and break
    if word in dictionary_area:
      break
  # Check if there's a word close to the input word
  if len(area) < 1:
      area = closest_area(utterance,dictionary_area)

  # Find food type
  food_type = []

  # Possible food types
  dictionary_food = ["british","european","italian","romanian","seafood","chinese",
                "steakhouse","asian","french","portugese","indian","spanish"
                "vietnamese","korean","thai","moroccan","swiss","fusion",
                "gastropub","tuscan","international","traditional","mediterranean","polynesian"
                "african","turkish","bistro","american","australasian","australasian",
                "persian","jamaican","lebanese","cuban","japanese","catalan"]
                
  for word in utterance:
    # If input word matches one in dictionary, then succesful and break
    if word in dictionary_food:
      food_type = word
      break
  # Check if there's a word close to the input word
  if len(food_type) < 1:
      food_type = closest_area(utterance,dictionary_food)

  # Find price
  price = []

  # Possible price ranges
  dictionary_price = ["moderate","expensive","cheap"]

  for word in utterance:
    # If input word matches one in dictionary, then succesful and break
    if word in dictionary_price:
      price = word
      break
  # Check if there's a word close to the input word
  if len(price) < 1:
      price = closest_area(utterance,dictionary_price)

  # Return found user preferences
  return area, food_type, price

# Preference function testing
# Prepare user input to find preferences
inpt = "Hi I am looking for the Nerth restaurant with food"

# Convert input to lowercase
inpt_lower = inpt.lower()

# Find keywords
area, food_type, price = keyword(inpt_lower.split())

# Recommnedation lookup function
import pandas as pd
import numpy as np
import random

# Find a matching recommendation from database restaurant_info.csv
def get_recommendations(area, food_type, price):
  df = pd.read_csv("restaurant_info.csv") 
  recommendations=df[(df.area == area) & (df.food==food_type) & (df.pricerange==price)]
  return recommendations

# Function to find type of dialog in a string based on keyword
def dialog_type(utterance):

  # Check the possible dialog_type keywords
  if utterance.find("okay") > 0 or utterance.find("oke") > 0:
    keyword = "ack"
  elif utterance.find("yes") > 0 or utterance.find("right") > 0:
    keyword = "affirm"
  elif utterance == "no":
    keyword = "negate"
  elif utterance.find("how about") > 0 or utterance.find("what about") > 0 or utterance.find("else") > 0:
    keyword = "reqalts"
  elif utterance.find("thank you") > 0 or utterance.find("thankyou") > 0 or utterance.find("thanks") > 0:
    keyword = "thankyou"
  elif utterance.find("bye") > 0:
    keyword = "bye"
  elif utterance.find("start") > 0:
    keyword = "restart"
  elif utterance.find("what") > 0 or utterance.find("how") > 0 or utterance.find("address") > 0 or utterance.find("number") > 0 or utterance.find("postcode") > 0:
    keyword = "request"
  elif utterance.find("more") > 0:
    keyword = "reqmore"
  elif utterance.find("repeat") > 0:
    keyword = "repeat"
  elif utterance.find("hi") > 0 or utterance.find("hello") > 0:
    keyword = "hello" 
  elif utterance.find("don't") > 0 or utterance.find("dont") > 0:
    keyword = "deny"
  elif utterance.find("is it") > 0 or utterance == "is":
    keyword = "confirm"
  elif utterance.find("looking") > 0 or utterance.find("want") > 0 or utterance.find("would") > 0 or utterance.find("any") > 0 or utterance.find("restaurant") > 0 or utterance.find("food") > 0:
    keyword = "inform"
  elif len(utterance) < 7 or utterance.find("noise") > 0 or utterance.find('sil') > 0:
    keyword = "null"
  else:
    keyword = "inform"

  return keyword

#States
# 1 preference request
# 2 ask for missing preferences
# 3 recommend restaurant based on preferences
# 4 listen for information requests
# 5 dialog close
# 6 cannot understand request
# 7 answer request

# Dialog system
state = 1
dialog_active = True


# Initiate preferences
area = None
food_type = None
price = None

recommendations = []

# Until user ends conversation, check for input and handle accordingly
while dialog_active:
  if state == 1:

  # Start the conversation with the welcome sentence
    print("Hello , welcome to the Cambridge restaurant system? You can ask for restaurants by area , price range or food type . How may I help you?")

  # Go to next state after asking the question
    state = 2

  # State 2 is the state to get the missing preferences and allows the user to update previous preferences
  while state == 2:
    
    # Get user input for missing preferences
    user_input = input()
    user_input = user_input.lower()
    new_area, new_food_type, new_price = keyword(user_input.split())

    # Update old preferences when new preference != None
    if new_area != None:
      area = new_area
    if new_food_type != None:
      food_type = new_food_type
    if new_price != None:
      price = new_price

    # If there is a preference left unspecified, ask user for preference
    if area == None:
      print("What part of town do you have in mind?")
    elif food_type == None:
      print("What kind of food would you like?")
    elif price == None:
      print("Would you like something in the cheap , moderate , or expensive price range?")
    # Go to recommendation state once all preferences have been given
    else:
        state = 3

  # State 3 is a one pass state that will send to user back to state 2 or 4 depending if on the recommendation
  if state == 3:

    # Result should be stored into recommendation (array)
    recommendations = get_recommendations(area, food_type, price)

    # If there is only 1 recommendation, recommend it and go to request state
    if len(recommendations) == 1:
      print("There is a restuarant called Name is %s that is located in %s and its prices are %s" %(recommendations['restaurantname'].values[0],recommendations['area'].values[0],price))
      state = 4

    # If there are multiple recommendations, recommend random one and go to request state
    elif len(recommendations) > 1:
      recommendations=recommendations.sample()
      print("There is a restuarant called Name is %s that is located in %s and its prices are %s" %(recommendations['restaurantname'].values[0],recommendations['area'].values[0],price))
      state = 4

    # If there are no recommendations, tell the user and allow for new preferences by going to state 2
    else:
      print("I'm sorry but there is no", price ,"priced restaurant in the", area, "serving", food_type, "food")
      state = 2

  # State 4 handles all the request from the user after a recommendation
  while state == 4:
    user_input = input()

    # Analyse user_input request
    input_type = dialog_type(user_input)

    # If the user input is a dialog_act bye statement, close the conversation
    if "bye" in user_input:
      state = 5
      dialog_active = False

    # Print request result, this can update the recommendation
    elif input_type == "request":

      # Check if user asks for phone number and answer
      if "number" in user_input:
        print("The phone number of this restaurant is ",recommendations['phone'].values[0])

      # Check if user asks for address and answer
      if "address" in user_input:
        print("The address of this restaurant is ",recommendations['addr'].values[0])    
  
    # If the system did not understand the request tell the user their selected restaurand and that it is nice
    else:
      print(recommendations['restaurantname'].values[0]," is a great restaurant ")