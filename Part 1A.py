# -*- coding: utf-8 -*-
"""MAIR Group 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GU5ZxJqwHSkRM6dDSaTw-rZN85aKHSK7

# Baseline systems
"""

# Import libraries 
import pandas as pd
import numpy as np
import sklearn.metrics
from tensorflow import keras
from sklearn.metrics import accuracy_score
from matplotlib import pyplot as plt
from mlxtend.evaluate import confusion_matrix
from mlxtend.plotting import plot_confusion_matrix
import random
import seaborn as sn
from io import StringIO
from collections import Counter
from sklearn import model_selection
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import log_loss
from sklearn.metrics import precision_recall_fscore_support as score, precision_score, recall_score, f1_score
from sklearn import preprocessing
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
from keras.models import Sequential 
from keras import layers
from keras.backend import clear_session
import seaborn as sns
from sklearn import metrics
from sklearn.metrics import precision_recall_fscore_support

## Section 1: import data
# Import data file
df = pd.read_csv(('dialog_acts.dat'),names=['label'])

# Transform to lowercase
df['label'] = df['label'].str.lower()
df[['label','utterance']] = df['label'].str.split(" ",1,expand=True)

# split into 85% train and 15% test data
mask = np.random.rand(len(df)) <= 0.85
training_data = df[mask]
testing_data = df[~mask]

## Section 2: Models
"""**Majority class baseline system**"""

# Function that uses testing data as input and classifies as most_common label
def majority_baseline(testing_data):
  # Copy testing_data dataframe
  df_labels = pd.DataFrame.copy(testing_data)
  # Find most common label
  most_common = testing_data['label'].value_counts().idxmax()

  # Fill all rows of df dataframe with most common label
  df_labels[:]['label'] = most_common
  print(testing_data)
  return df_labels['label']

# compare predicted labels with correct labels in testing_data
accuracy_score(testing_data['label'],majority_baseline(testing_data))

"""**Rule based baseline system**"""

# Function that uses testing_data as input and classifies using keyword matching
def keyword_matching(testing_data):
  # Copy testing_data dataframe
  df_labels = pd.DataFrame.copy(testing_data)
  # Create empty array for storing predicted labels
  keyword_label = []

  # Iterate through testing_data dataframe and check every row's keyword
  for row in range(len(df_labels)):
    word = str(df_labels['utterance'].iloc[[row]])
    keyword_label.append(dialog_type(word))

  # Fill df_labels dataframe with predicted labels
  df_labels[:]['label'] = keyword_label
  return df_labels['label']

# Function to find type of dialog in a string - called by keyword_matching
def dialog_type(utterance):
  if utterance.find("okay") > 0 or utterance.find("oke") > 0:
    keyword = "ack"
  elif utterance.find("yes") > 0 or utterance.find("right") > 0:
    keyword = "affirm"
  elif utterance == "no":
    keyword = "negate"
  elif utterance.find("how about") > 0 or utterance.find("what about") > 0 or utterance.find("else") > 0:
    keyword = "reqalts"
  elif utterance.find("thank you") > 0 or utterance.find("thankyou") > 0 or utterance.find("thanks") > 0:
    keyword = "thankyou"
  elif utterance.find("bye") > 0:
    keyword = "bye"
  elif utterance.find("start") > 0:
    keyword = "restart"
  elif utterance.find("what") > 0 or utterance.find("how") > 0 or utterance.find("address") > 0 or utterance.find("number") > 0 or utterance.find("postcode") > 0:
    keyword = "request"
  elif utterance.find("more") > 0:
    keyword = "reqmore"
  elif utterance.find("repeat") > 0:
    keyword = "repeat"
  elif utterance.find("hi") > 0 or utterance.find("hello") > 0:
    keyword = "hello" 
  elif utterance.find("don't") > 0 or utterance.find("dont") > 0:
    keyword = "deny"
  elif utterance.find("is it") > 0 or utterance == "is":
    keyword = "confirm"
  elif utterance.find("looking") > 0 or utterance.find("want") > 0 or utterance.find("would") > 0 or utterance.find("any") > 0 or utterance.find("restaurant") > 0 or utterance.find("food") > 0:
    keyword = "inform"
  elif len(utterance) < 7 or utterance.find("noise") > 0 or utterance.find('sil') > 0:
    keyword = "null"
  else:
    keyword = "inform"
  return keyword

# compare predicted labels with correct labels in testing_data
print(accuracy_score(testing_data['label'],keyword_matching(testing_data)))

# All possible response types (15 types)
list_labels = ["ack","affirm","negate","inform","thankyou","bye", "restart","request","reqmore",  "reqalts", "repeat", "hello" ,"deny","confirm", "null"]

# Compare the prediction of keyword matching model with actual labels
# Get keyword model's prediction
pred_key = keyword_matching(testing_data)

# Actual labels
test_key = testing_data['label']

# Sentences of testing_data
inpt = testing_data['utterance']

# Compare sentence by sentence if a prediction was correct
for row_index, (inpt, prediction, label) in enumerate( zip (inpt, pred_key, test_key)):
  # If prediction incorrect, print the sentence and misclassification type
  if prediction != label:
    print('Row', row_index, 'has been classified as ', prediction, 'and should be ', label, '\t', '\t', inpt)

    
"""#Machine learning models

## Model 1: Neural Network
"""
# Create neural network model
# Copy train and test data for this model
m1_sentences_train = training_data['utterance']
m1_sentences_test = testing_data['utterance']
m1_y_train = training_data['label']
m1_y_test = testing_data['label']

# Vectorize the training and testing x data
m1_vectorizer = CountVectorizer()
m1_vectorizer.fit(m1_sentences_train)
m1_x_train = m1_vectorizer.transform(m1_sentences_train)
m1_x_test = m1_vectorizer.transform(m1_sentences_test)

# Categorize the labels (y data)
label_encoder = LabelEncoder()
m1_y_train = label_encoder.fit_transform(m1_y_train)
m1_y_test = label_encoder.fit_transform(m1_y_test)
m1_y_train = keras.utils.to_categorical(m1_y_train, 18)
m1_y_test = keras.utils.to_categorical(m1_y_test, 18)

# Test neural network
m1_input_dim = m1_x_train.shape[1]

# Architecture - layers
model1 = Sequential()
model1.add(layers.Dense(28, input_dim=m1_input_dim, activation = 'relu'))
model1.add(layers.Dense(18, activation='sigmoid'))
model1.compile(loss='binary_crossentropy', optimizer='adam', metrics='accuracy')

# Model summary
model1.summary()

# Train model and print training epochs
history = model1.fit(m1_x_train, m1_y_train, epochs=15, verbose=1 , validation_data=(m1_x_test, m1_y_test), batch_size=100)

# Get training and validation accuracy for neural network
model1_training_loss, model1_training_accuracy = model1.evaluate(m1_x_train, m1_y_train, verbose=False)
print("Training Accuracy: ",  "{:.0%}".format(model1_training_accuracy), " Training loss: ", "{:.0%}". format(model1_training_loss))
model1_validation_loss, model1_validation_accuracy = model1.evaluate(m1_x_test, m1_y_test, verbose=False)
print("Validation Accuracy: ", "{:.0%}".format(model1_validation_accuracy), " Validation loss: ", "{:.0%}". format(model1_validation_loss))

# Necessary for evaluation later on
m1_y_pred=model1.predict(m1_x_test) 
m1_y_pred=np.argmax(m1_y_pred, axis=1)
y_test_eva =np.argmax(m1_y_test, axis=1)

# Plot the training (validation) accuracy and history
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Acuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='right')
plt.show()

# Plot training loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='right')
plt.show()

# Calculate loss and accuracy
loss, accuracy = model1.evaluate(m1_x_test, m1_y_test, verbose=0)
print(loss)
print(accuracy)

# Input prompt for the first model - uncomment if want to test with new data
'''
while(True):
  pred_sentence = str(input("Enter new sentence : "))
  pred_sentence = m1_vectorizer.transform([pred_sentence])
  y_new = model1.predict([pred_sentence])
  y_class = y_new.argmax(axis=-1)
  print(sorted(list_labels)[y_class[0]])
'''

"""## Model 2: Logistic Regression"""

# Model 2: Logistic regression
# Prepare data - vectorize
m2_sentences = df['utterance'].values
m2_y = df['label'].values
m2_sentences_train, m2_sentences_test, m2_y_train, m2_y_test = train_test_split(m2_sentences, m2_y, test_size=0.15, random_state=1000)
m2_vectorizer = CountVectorizer()
m2_vectorizer.fit(m2_sentences_train)
m2_x_train = m2_vectorizer.transform(m2_sentences_train)
m2_x_test  = m2_vectorizer.transform(m2_sentences_test)

# Fit regression model
logisticRegr = LogisticRegression(max_iter = 250)
logisticRegr.fit(m2_x_train, m2_y_train)

# Predict one input point
logisticRegr.predict(m2_x_test[0].reshape(1,-1))
logisticRegr.predict(m2_x_test[0:10])
predictions = logisticRegr.predict(m2_x_test)

# Get accuracy
score = logisticRegr.score(m2_x_test, m2_y_test)
print(score)

# Input prompt for the second model - uncomment if want to test with new data
'''
while(True):
  pred_sentence = input("Enter new sentence : ")
  pred_sentence = m2_vectorizer.transform([pred_sentence])
  y_class = logisticRegr.predict(pred_sentence)
  print(y_class)
'''

"""# Evaluation
"""

"""## Model 1: Neural network
### Quantitative evaluation
We chose the accuracy as a general overview for quantitive evaluation because it gives a good estimate of how well the model performs. 
Also the F1 score because if the data is heavily skewed, then the F1 gives a good estimate.
"""

# Neural network quantative evaluation
# plot training history
plt.figure(figsize=(9,6))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy Vs Epochs')
plt.ylabel('Accuracy')
plt.xlabel('Number of Epochs')
plt.legend(['Training Accuracy', 'Validation Accuracy'])
plt.show()

# Calculate training and validation accuracy
model1_training_loss, model1_training_accuracy = model1.evaluate(m1_x_train, m1_y_train, verbose=False)
print("Training Accuracy: ",  "{:.0%}".format(model1_training_accuracy), " Training loss: ", "{:.0%}". format(model1_training_loss))

model1_validation_loss, model1_validation_accuracy = model1.evaluate(m1_x_test, m1_y_test, verbose=False)
print("Validation Accuracy: ", "{:.0%}".format(model1_validation_accuracy), " Validation loss: ", "{:.0%}". format(model1_validation_loss))

# Confusion Matrix to see where things go wrong
print("Confusion Matrix of Model 1")
cm = confusion_matrix(y_test_eva, m1_y_pred)
plt.figure(figsize = (15,12))
sn.heatmap(cm, annot=True,cmap="OrRd",fmt='g')
# fig, ax = plot_confusion_matrix(cm, figsize=(8,8),)
# plt.title('Confusion Matrix')
# plt.show()

# Calculate F-score - precision and recall
precision_recall_fscore_support(y_test_eva, m1_y_pred, average='macro')

"""So precision is 0.63 and recall is 0.60 percent. This is not very high as we will see later on in comparison with the logistic regression.

### Error analysis
"""
# Finding items that are most often classified wrong
inpt = testing_data['utterance']
wrong_utterance = [] 
for row_index, (inpt, prediction, label) in enumerate( zip (inpt, m1_y_pred, y_test_eva)):
  if prediction != label:
    wrong_utterance.append(inpt)

Counter(wrong_utterance).most_common()[0:15]

"""The Model 1 Neural network classifier finds it hard to classify 'okay thank you and good bye' which is understandable since it contains 3 different commands: a confirmation, thankyou and goodbye. Especially commands containing a form of confirmation 'okay' without being a confirmation are hard.
It also classifies a couple of other cases wrong, but since the accuracy is really high, these are just single instances.

### Difficult cases
***TODO: for example utterances that are not fluent (e.g. due to speech recognition issues) or the presence of negation (I don’t want an expensive restaurant). For each case, create test instances and evaluate how your systems perform on these cases. Such as:***

### System comparison
The neural network model compares well against the baseline. The validation accuracy is 98 percent compared to 39 percent majority and 84 percent keyword baseline systems. Therefore, the neural network is much more accurate than the baseline systems with an improvement of at least 14 percent accuracy. For the comparison of a neural network model 1 and model 2 logistic regression, see the model 2 section on system comparison.

## Model 2: Logistic Regression
### Quantative evaluation
"""

# Create confusion matrix
cm = metrics.confusion_matrix(m2_y_test, predictions)
plt.figure(figsize = (15,12))
sn.heatmap(cm, annot=True,cmap="OrRd",fmt='g')

# Calculate F1 score - precision and recall
precision_recall_fscore_support(m2_y_test, predictions, average='macro')

"""So precision is 0.91 and recall is 0.81 percent.

### Error analysis
"""

#Finding items that are most often classified wrong
inpt = testing_data['utterance']
wrong_utterance = [] 
for row_index, (inpt, prediction, label) in enumerate( zip (inpt, predictions, m2_y_test)):
  if prediction != label:
    wrong_utterance.append(inpt)

Counter(wrong_utterance).most_common()[0:15]

"""The model 2 logistic regression classifies the 'i don't care' and the short answers containing one word often wrong. It also finds it hard to classify the sentence containing both a thankyou and goodbye, just like model 1. It can be imagined that this is indeed difficult for a model because there are two commands and thus the sentence classifies equally well as both categories.

### Difficult cases

### System comparison
The logistic regression performs well compared to the baseline models. Both baseline models have lower accuracy and are therefore not great classifiers. 
The logistic regression reaches an accuracy of 98 percent which is a good performance. 
This model would be better suited to classify dialog than the baseline models.

The neural network has the same accuracy as the logistic regression model, with both having a 98 percent prediction accuracy in classifying dialog. 
Interestingly, both models also both have difficulty classifying sentences containing two commands. 
However, when we look at the precision and recall score resulting from the F1 analysis, it turns out that the logistic regression has a much higher 
percentage in both precision and recall than the neural network. 
Therefore, we conclude that the logistic regression would be better suited to classify dialog than the neural network.  TODO: add difficult cases evaluation
"""

# Difficult cases test
names = ['ack']
dataframe = pd.read_csv('dialog_acts.dat', names=names)
array = dataframe.values
test_size = 0.33
X_train, X_test, Y_train, Y_test = train_test_split(dataframe,array, train_size = 0.85)
Y_test= Y_test['ack']
print(Y_test.head())
# model = LogisticRegression(solver='liblinear')
# model= preprocessing.LabelEncoder()
# model.fit_transform(X_train, Y_train)
# predicted = model.predict(X_test)
# report = classification_report(Y_test, predicted)
# print(report)
